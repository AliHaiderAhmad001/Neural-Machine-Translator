{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Download Dataset"
      ],
      "metadata": {
        "id": "PtxtWv4pARNS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Colab Notebooks/NMT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bKSgofzA1n6",
        "outputId": "7a196244-d2d9-4f2d-a1e2-6dbe35087985"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/NMT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjoMUIa6AJPb",
        "outputId": "6ca50dc6-4602-4d09-8196-b88ee01f4920"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/fra-eng.zip\n",
            "3423204/3423204 [==============================] - 0s 0us/step\n",
            "/root/.keras/datasets/fra.txt\n"
          ]
        }
      ],
      "source": [
        "import pathlib\n",
        "import tensorflow as tf\n",
        "# download dataset provided by Anki: https://www.manythings.org/anki/\n",
        "text_file = tf.keras.utils.get_file(\n",
        "    fname=\"fra-eng.zip\",\n",
        "    origin=\"http://storage.googleapis.com/download.tensorflow.org/data/fra-eng.zip\",\n",
        "    extract=True,\n",
        ")\n",
        "# show where the file is located now\n",
        "text_file = pathlib.Path(text_file).parent / \"fra.txt\"\n",
        "print(text_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "6sdTvZlCA7YX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import random\n",
        "import re\n",
        "import unicodedata\n",
        "\n",
        "def normalize(line):\n",
        "    \"\"\"Normalize a line of text and split into two at the tab character\"\"\"\n",
        "    line = unicodedata.normalize(\"NFKC\", line.strip().lower())\n",
        "    line = re.sub(r\"^([^ \\w])(?!\\s)\", r\"\\1 \", line)\n",
        "    line = re.sub(r\"(\\s[^ \\w])(?!\\s)\", r\"\\1 \", line)\n",
        "    line = re.sub(r\"(?!\\s)([^ \\w])$\", r\" \\1\", line)\n",
        "    line = re.sub(r\"(?!\\s)([^ \\w]\\s)\", r\" \\1\", line)\n",
        "    eng, fra = line.split(\"\\t\")\n",
        "    fra = \"[start] \" + fra + \" [end]\"\n",
        "    return eng, fra\n",
        "\n",
        "# normalize each line and separate into English and French\n",
        "with open(text_file) as fp:\n",
        "    text_pairs = [normalize(line) for line in fp]\n",
        "\n",
        "# print some samples\n",
        "for _ in range(5):\n",
        "    print(random.choice(text_pairs))\n",
        "\n",
        "with open(\"text_pairs.pickle\", \"wb\") as fp:\n",
        "    pickle.dump(text_pairs, fp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MsbOAiQFBBxr",
        "outputId": "79b0c656-1c9c-491a-a268-1e360d988921"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('my wife is obsessed with cleaning .', '[start] ma femme est obsédée par le nettoyage . [end]')\n",
            "('are you happy in your house ?', '[start] êtes-vous heureux dans votre maison  ?  [end]')\n",
            "('did you already do your homework ?', '[start] as-tu déjà fini tes devoirs  ?  [end]')\n",
            "('the cause of the accident is not known to us .', \"[start] la cause de l'accident nous est inconnue . [end]\")\n",
            "('the chemical formula for water is h2o .', \"[start] la formule chimique de l'eau est h2o . [end]\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"text_pairs.pickle\", \"rb\") as fp:\n",
        "    text_pairs = pickle.load(fp)\n",
        "\n",
        "# count tokens\n",
        "eng_tokens, fra_tokens = set(), set()\n",
        "eng_maxlen, fra_maxlen = 0, 0\n",
        "for eng, fra in text_pairs:\n",
        "    eng_tok, fra_tok = eng.split(), fra.split()\n",
        "    eng_maxlen = max(eng_maxlen, len(eng_tok))\n",
        "    fra_maxlen = max(fra_maxlen, len(fra_tok))\n",
        "    eng_tokens.update(eng_tok)\n",
        "    fra_tokens.update(fra_tok)\n",
        "print(f\"Total English tokens: {len(eng_tokens)}\")\n",
        "print(f\"Total French tokens: {len(fra_tokens)}\")\n",
        "print(f\"Max English length: {eng_maxlen}\")\n",
        "print(f\"Max French length: {fra_maxlen}\")\n",
        "print(f\"{len(text_pairs)} total pairs\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSyTWcklHOPk",
        "outputId": "70080aee-ac26-4b25-a9b7-040ee8169cb0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total English tokens: 14969\n",
            "Total French tokens: 29219\n",
            "Max English length: 51\n",
            "Max French length: 60\n",
            "167130 total pairs\n"
          ]
        }
      ]
    }
  ]
}