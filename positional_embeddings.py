# -*- coding: utf-8 -*-
"""positional_embeddings.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iAjbOC_0Y8t2SafAmfKp29Dkrn-7T56q
"""

import tensorflow as tf

class SinusoidalPositionalEncoding(tf.keras.layers.Layer):
    """
    SinusoidalPositionalEncoding layer.

    This layer applies sinusoidal positional encodings to the input embeddings.

    Args:
        config (object): Configuration object containing parameters.
    """
    def __init__(self, config, name = None, **kwargs):
        """
        Initialize the SinusoidalPositionalEncoding layer.

        Args:
            config (object): Configuration object with parameters for positional encoding.
        """
        super(SinusoidalPositionalEncoding, self).__init__(name = name)
        super(SinusoidalPositionalEncoding, self).__init__(**kwargs)

    def call(self, input_ids):
        """
        Apply positional encodings to the input embeddings.

        Args:
            input_ids (tf.Tensor): Input tensor containing token IDs.

        Returns:
            tf.Tensor: Output tensor with positional encodings added.
        """
        self.position_encoding = self.create_positional_encoding_matrix(
                input_ids.shape[1], config.hidden_size, config.frequency_factor
        )
        return self.position_encoding

    def create_positional_encoding_matrix(self, sequence_length, embedding_dimension, frequency_factor=10000):
        """
        Create a positional encoding matrix.

        Args:
            sequence_length (int): Length of the input sequence.
            embedding_dimension (int): Dimensionality of the positional embeddings. Must be an even integer.
            frequency_factor (int): Constant for the sinusoidal functions.

        Returns:
            tf.Tensor: Matrix of positional embeddings of shape (sequence_length, embedding_dimension).
            The value at element (k, 2i) is sin(k / frequency_factor^(2i / embedding_dimension)),
            and the value at element (k, 2i+1) is cos(k / frequency_factor^(2i / embedding_dimension)).
        """
        assert embedding_dimension % 2 == 0, "Embedding dimension needs to be an even integer"
        embedding_dimension_half = embedding_dimension // 2
        positions = tf.range(sequence_length, dtype=tf.float32)[:, tf.newaxis]  # Column vector of shape (sequence_length, 1)
        frequency_indices = tf.range(embedding_dimension_half, dtype=tf.float32)[tf.newaxis, :]  # Row vector of shape (1, embedding_dimension/2)
        frequency_denominator = tf.pow(frequency_factor, -frequency_indices / embedding_dimension_half)  # frequency_factor^(-2i/d)
        frequency_arguments = positions / frequency_denominator  # Matrix of shape (sequence_length, embedding_dimension)
        sin_values = tf.sin(frequency_arguments)
        cos_values = tf.cos(frequency_arguments)
        positional_encodings = tf.concat([sin_values, cos_values], axis=1)

        return positional_encodings

    def get_config(self):
        """
        Get the configuration of the SinusoidalPositionalEncoding layer.

        Returns:
            dict: Configuration dictionary.
        """
        config = super().get_config()
        return config


class PositionalEmbeddings(tf.keras.layers.Layer):
    """
    PositionalEmbeddings layer.

    This layer generates positional embeddings based on input IDs.
    It uses an Embedding layer to map position IDs to position embeddings.

    Args:
        config (object): Configuration object containing parameters.
    """

    def __init__(self, config, name = None, **kwargs):
        super(PositionalEmbeddings, self).__init__(name = name)
        super(PositionalEmbeddings, self).__init__(**kwargs)
        self.positional_embeddings = tf.keras.layers.Embedding(
            input_dim=config.sequence_length, output_dim=config.hidden_size
        )

    def call(self, input_ids):
        """
        Generate positional embeddings.

        Args:
            input_ids (tf.Tensor): Input tensor containing token IDs.

        Returns:
            tf.Tensor: Positional embeddings tensor of shape (batch_size, seq_length, hidden_size).
        """
        seq_length = input_ids.shape[1]
        position_ids = tf.range(seq_length, dtype=tf.int32)[tf.newaxis, :]
        position_embeddings = self.positional_embeddings(position_ids)
        return position_embeddings

    def get_config(self):
        """
        Get the layer configuration.

        Returns:
            dict: Dictionary containing the layer configuration.
        """
        config = super().get_config()
        config.update({
            "positional_embeddings": self.positional_embeddings,
        })
        return config