{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Download Dataset"
      ],
      "metadata": {
        "id": "PtxtWv4pARNS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Colab Notebooks/NMT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bKSgofzA1n6",
        "outputId": "7a196244-d2d9-4f2d-a1e2-6dbe35087985"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/NMT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fjoMUIa6AJPb"
      },
      "outputs": [],
      "source": [
        "import pathlib\n",
        "import tensorflow as tf\n",
        "# download dataset provided by Anki: https://www.manythings.org/anki/\n",
        "text_file = tf.keras.utils.get_file(\n",
        "    fname=\"fra-eng.zip\",\n",
        "    origin=\"http://storage.googleapis.com/download.tensorflow.org/data/fra-eng.zip\",\n",
        "    extract=True,\n",
        ")\n",
        "# show where the file is located now\n",
        "text_file = pathlib.Path(text_file).parent / \"fra.txt\"\n",
        "print(text_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "6sdTvZlCA7YX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import random\n",
        "import re\n",
        "import unicodedata\n",
        "\n",
        "def normalize(line):\n",
        "    \"\"\"Normalize a line of text and split into two at the tab character\"\"\"\n",
        "    line = unicodedata.normalize(\"NFKC\", line.strip().lower())\n",
        "    line = re.sub(r\"^([^ \\w])(?!\\s)\", r\"\\1 \", line)\n",
        "    line = re.sub(r\"(\\s[^ \\w])(?!\\s)\", r\"\\1 \", line)\n",
        "    line = re.sub(r\"(?!\\s)([^ \\w])$\", r\" \\1\", line)\n",
        "    line = re.sub(r\"(?!\\s)([^ \\w]\\s)\", r\" \\1\", line)\n",
        "    eng, fra = line.split(\"\\t\")\n",
        "    fra = \"[start] \" + fra + \" [end]\"\n",
        "    return eng, fra\n",
        "\n",
        "# normalize each line and separate into English and French\n",
        "with open(text_file) as fp:\n",
        "    text_pairs = [normalize(line) for line in fp]\n",
        "\n",
        "# print some samples\n",
        "for _ in range(5):\n",
        "    print(random.choice(text_pairs))\n",
        "\n",
        "with open(\"text_pairs.pickle\", \"wb\") as fp:\n",
        "    pickle.dump(text_pairs, fp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MsbOAiQFBBxr",
        "outputId": "79b0c656-1c9c-491a-a268-1e360d988921"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('my wife is obsessed with cleaning .', '[start] ma femme est obsédée par le nettoyage . [end]')\n",
            "('are you happy in your house ?', '[start] êtes-vous heureux dans votre maison  ?  [end]')\n",
            "('did you already do your homework ?', '[start] as-tu déjà fini tes devoirs  ?  [end]')\n",
            "('the cause of the accident is not known to us .', \"[start] la cause de l'accident nous est inconnue . [end]\")\n",
            "('the chemical formula for water is h2o .', \"[start] la formule chimique de l'eau est h2o . [end]\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"text_pairs.pickle\", \"rb\") as fp:\n",
        "    text_pairs = pickle.load(fp)\n",
        "\n",
        "# count tokens\n",
        "eng_tokens, fra_tokens = set(), set()\n",
        "eng_maxlen, fra_maxlen = 0, 0\n",
        "for eng, fra in text_pairs:\n",
        "    eng_tok, fra_tok = eng.split(), fra.split()\n",
        "    eng_maxlen = max(eng_maxlen, len(eng_tok))\n",
        "    fra_maxlen = max(fra_maxlen, len(fra_tok))\n",
        "    eng_tokens.update(eng_tok)\n",
        "    fra_tokens.update(fra_tok)\n",
        "print(f\"Total English tokens: {len(eng_tokens)}\")\n",
        "print(f\"Total French tokens: {len(fra_tokens)}\")\n",
        "print(f\"Max English length: {eng_maxlen}\")\n",
        "print(f\"Max French length: {fra_maxlen}\")\n",
        "print(f\"{len(text_pairs)} total pairs\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSyTWcklHOPk",
        "outputId": "70080aee-ac26-4b25-a9b7-040ee8169cb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total English tokens: 14969\n",
            "Total French tokens: 29219\n",
            "Max English length: 51\n",
            "Max French length: 60\n",
            "167130 total pairs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "with open(\"text_pairs.pickle\", \"rb\") as fp:\n",
        "    text_pairs = pickle.load(fp)\n",
        "\n",
        "# histogram of sentence length in tokens\n",
        "en_lengths = [len(eng.split()) for eng, fra in text_pairs]\n",
        "fr_lengths = [len(fra.split()) for eng, fra in text_pairs]\n",
        "\n",
        "plt.hist(en_lengths, label=\"en\", color=\"red\", alpha=0.33)\n",
        "plt.hist(fr_lengths, label=\"fr\", color=\"blue\", alpha=0.33)\n",
        "plt.yscale(\"log\")     # sentence length fits Benford\"s law\n",
        "plt.ylim(plt.ylim())  # make y-axis consistent for both plots\n",
        "plt.plot([max(en_lengths), max(en_lengths)], plt.ylim(), color=\"red\")\n",
        "plt.plot([max(fr_lengths), max(fr_lengths)], plt.ylim(), color=\"blue\")\n",
        "plt.legend()\n",
        "plt.title(\"Examples count vs Token length\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "qlPNd6yrHsmF",
        "outputId": "5daf5aa9-8a58-4aca-b3ca-bd97c3eedcb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vectorization and Making Datasets"
      ],
      "metadata": {
        "id": "-ZkBwvWnIHrz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "# Load normalized sentence pairs\n",
        "with open(\"text_pairs.pickle\", \"rb\") as fp:\n",
        "    text_pairs = pickle.load(fp)\n",
        "\n",
        "# train-test-val split of randomized sentence pairs\n",
        "random.shuffle(text_pairs)\n",
        "n_val = int(0.15*len(text_pairs))\n",
        "n_train = len(text_pairs) - 2*n_val\n",
        "train_pairs = text_pairs[:n_train]\n",
        "val_pairs = text_pairs[n_train:n_train+n_val]\n",
        "test_pairs = text_pairs[n_train+n_val:]\n",
        "\n",
        "# Parameter determined after analyzing the input data\n",
        "vocab_size_en = 10000\n",
        "vocab_size_fr = 20000\n",
        "seq_length = 60\n",
        "\n",
        "# Create vectorizer\n",
        "eng_vectorizer = TextVectorization(\n",
        "    max_tokens=vocab_size_en,\n",
        "    standardize=None,\n",
        "    split=\"whitespace\",\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=seq_length,\n",
        ")\n",
        "fra_vectorizer = TextVectorization(\n",
        "    max_tokens=vocab_size_fr,\n",
        "    standardize=None,\n",
        "    split=\"whitespace\",\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=seq_length + 1\n",
        ")\n",
        "\n",
        "# train the vectorization layer using training dataset\n",
        "train_eng_texts = [pair[0] for pair in train_pairs]\n",
        "train_fra_texts = [pair[1] for pair in train_pairs]\n",
        "eng_vectorizer.adapt(train_eng_texts)\n",
        "fra_vectorizer.adapt(train_fra_texts)\n",
        "\n",
        "# save for subsequent steps\n",
        "with open(\"vectorize.pickle\", \"wb\") as fp:\n",
        "    data = {\n",
        "        \"train\": train_pairs,\n",
        "        \"val\":   val_pairs,\n",
        "        \"test\":  test_pairs,\n",
        "        \"engvec_config\":  eng_vectorizer.get_config(),\n",
        "        \"engvec_weights\": eng_vectorizer.get_weights(),\n",
        "        \"fravec_config\":  fra_vectorizer.get_config(),\n",
        "        \"fravec_weights\": fra_vectorizer.get_weights(),\n",
        "    }\n",
        "    pickle.dump(data, fp)"
      ],
      "metadata": {
        "id": "tRDKwDIiIJoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load text data and vectorizer weights\n",
        "with open(\"vectorize.pickle\", \"rb\") as fp:\n",
        "    data = pickle.load(fp)\n",
        "\n",
        "train_pairs = data[\"train\"]\n",
        "val_pairs = data[\"val\"]\n",
        "test_pairs = data[\"test\"]   # not used\n",
        "\n",
        "eng_vectorizer = TextVectorization.from_config(data[\"engvec_config\"])\n",
        "eng_vectorizer.set_weights(data[\"engvec_weights\"])\n",
        "fra_vectorizer = TextVectorization.from_config(data[\"fravec_config\"])\n",
        "fra_vectorizer.set_weights(data[\"fravec_weights\"])\n",
        "\n",
        "# set up Dataset object\n",
        "def format_dataset(eng, fra):\n",
        "    \"\"\"Take an English and a French sentence pair, convert into input and target.\n",
        "    The input is a dict with keys `encoder_inputs` and `decoder_inputs`, each\n",
        "    is a vector, corresponding to English and French sentences respectively.\n",
        "    The target is also vector of the French sentence, advanced by 1 token. All\n",
        "    vector are in the same length.\n",
        "\n",
        "    The output will be used for training the transformer model. In the model we\n",
        "    will create, the input tensors are named `encoder_inputs` and `decoder_inputs`\n",
        "    which should be matched to the keys in the dictionary for the source part\n",
        "    \"\"\"\n",
        "    eng = eng_vectorizer(eng)\n",
        "    fra = fra_vectorizer(fra)\n",
        "    source = {\"encoder_inputs\": eng,\n",
        "              \"decoder_inputs\": fra[:, :-1]}\n",
        "    target = fra[:, 1:]\n",
        "    return (source, target)\n",
        "\n",
        "def make_dataset(pairs, batch_size=64):\n",
        "    \"\"\"Create TensorFlow Dataset for the sentence pairs\"\"\"\n",
        "    # aggregate sentences using zip(*pairs)\n",
        "    eng_texts, fra_texts = zip(*pairs)\n",
        "    # convert them into list, and then create tensors\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((list(eng_texts), list(fra_texts)))\n",
        "    return dataset.shuffle(2048) \\\n",
        "                  .batch(batch_size).map(format_dataset) \\\n",
        "                  .prefetch(16).cache()\n",
        "\n",
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)\n",
        "\n",
        "# test the dataset\n",
        "for inputs, targets in train_ds.take(1):\n",
        "    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n",
        "    print(f'inputs[\"encoder_inputs\"][0]: {inputs[\"encoder_inputs\"][0]}')\n",
        "    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n",
        "    print(f'inputs[\"decoder_inputs\"][0]: {inputs[\"decoder_inputs\"][0]}')\n",
        "    print(f\"targets.shape: {targets.shape}\")\n",
        "    print(f\"targets[0]: {targets[0]}\")"
      ],
      "metadata": {
        "id": "mr46_YKpJDWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Positional Encoding Matrix"
      ],
      "metadata": {
        "id": "8rNdpyGWIt3s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "def pos_enc_matrix(L, d, n=10000):\n",
        "    \"\"\"Create positional encoding matrix\n",
        "\n",
        "    Args:\n",
        "        L: Input dimension (length)\n",
        "        d: Output dimension (depth), even only\n",
        "        n: Constant for the sinusoidal functions\n",
        "\n",
        "    Returns:\n",
        "        numpy matrix of floats of dimension L-by-d. At element (k,2i) the value\n",
        "        is sin(k/n^(2i/d)) while at element (k,2i+1) the value is cos(k/n^(2i/d))\n",
        "    \"\"\"\n",
        "    assert d % 2 == 0, \"Output dimension needs to be an even integer\"\n",
        "    d2 = d//2\n",
        "    P = np.zeros((L, d))\n",
        "    k = np.arange(L).reshape(-1, 1)     # L-column vector\n",
        "    i = np.arange(d2).reshape(1, -1)    # d-row vector\n",
        "    denom = np.power(n, -i/d2)          # n**(-2*i/d)\n",
        "    args = k / denom                    # (L,d) matrix\n",
        "    P[:, ::2] = np.sin(args)\n",
        "    P[:, 1::2] = np.cos(args)\n",
        "    return P\n",
        "\n",
        "class PositionalEmbedding(tf.keras.layers.Layer):\n",
        "    \"\"\"Positional embedding layer. Assume tokenized input, transform into\n",
        "    embedding and returns positional-encoded output.\"\"\"\n",
        "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            sequence_length: Input sequence length\n",
        "            vocab_size: Input vocab size, for setting up embedding matrix\n",
        "            embed_dim: Embedding vector size, for setting up embedding matrix\n",
        "        \"\"\"\n",
        "        super().__init__(**kwargs)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim     # d_model in paper\n",
        "        # token embedding layer: Convert integer token to D-dim float vector\n",
        "        self.token_embeddings = tf.keras.layers.Embedding(\n",
        "            input_dim=vocab_size, output_dim=embed_dim, mask_zero=True\n",
        "        )\n",
        "        # positional embedding layer: a matrix of hard-coded sine values\n",
        "        matrix = pos_enc_matrix(sequence_length, embed_dim)\n",
        "        self.position_embeddings = tf.constant(matrix, dtype=\"float32\")\n",
        "\n",
        "    def call(self, inputs):\n",
        "        \"\"\"Input tokens convert into embedding vectors then superimposed\n",
        "        with position vectors\"\"\"\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        return embedded_tokens + self.position_embeddings\n",
        "\n",
        "    # this layer is using an Embedding layer, which can take a mask\n",
        "    # see https://www.tensorflow.org/guide/keras/masking_and_padding#passing_mask_tensors_directly_to_layers\n",
        "    def compute_mask(self, *args, **kwargs):\n",
        "        return self.token_embeddings.compute_mask(*args, **kwargs)\n",
        "\n",
        "    def get_config(self):\n",
        "        # to make save and load a model using custom layer possible\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"sequence_length\": self.sequence_length,\n",
        "            \"vocab_size\": self.vocab_size,\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "        })\n",
        "        return config"
      ],
      "metadata": {
        "id": "I-gb-LP0IuP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer Building Blocks"
      ],
      "metadata": {
        "id": "plQuKWBfJas8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### self-attention model"
      ],
      "metadata": {
        "id": "LRqJrK89Q1-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def self_attention(input_shape, prefix=\"att\", mask=False, **kwargs):\n",
        "    \"\"\"Self-attention layers at transformer encoder and decoder. Assumes its\n",
        "    input is the output from positional encoding layer.\n",
        "\n",
        "    Args:\n",
        "        prefix (str): The prefix added to the layer names\n",
        "        masked (bool): whether to use causal mask. Should be False on encoder and\n",
        "                       True on decoder. When True, a mask will be applied such that\n",
        "                       each location only has access to the locations before it.\n",
        "    \"\"\"\n",
        "    # create layers\n",
        "    inputs = tf.keras.layers.Input(shape=input_shape, dtype='float32',\n",
        "                                   name=f\"{prefix}_in1\")\n",
        "    attention = tf.keras.layers.MultiHeadAttention(name=f\"{prefix}_attn1\", **kwargs)\n",
        "    norm = tf.keras.layers.LayerNormalization(name=f\"{prefix}_norm1\")\n",
        "    add = tf.keras.layers.Add(name=f\"{prefix}_add1\")\n",
        "    # functional API to connect input to output\n",
        "    attout = attention(query=inputs, value=inputs, key=inputs,\n",
        "                       use_causal_mask=mask)\n",
        "    outputs = norm(add([inputs, attout]))\n",
        "    # create model and return\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name=f\"{prefix}_att\")\n",
        "    return model\n",
        "\n",
        "seq_length = 20\n",
        "key_dim = 128\n",
        "num_heads = 8\n",
        "\n",
        "model = self_attention(input_shape=(seq_length, key_dim),\n",
        "                       num_heads=num_heads, key_dim=key_dim)\n",
        "tf.keras.utils.plot_model(model, \"self-attention.png\",\n",
        "                          show_shapes=True, show_dtype=True, show_layer_names=True,\n",
        "                          rankdir='BT', show_layer_activations=True)"
      ],
      "metadata": {
        "id": "JV94NuRHJBWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cross-attention model "
      ],
      "metadata": {
        "id": "iM5oGmmUQu3-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def cross_attention(input_shape, context_shape, prefix=\"att\", **kwargs):\n",
        "    \"\"\"Cross-attention layers at transformer decoder. Assumes its\n",
        "    input is the output from positional encoding layer at decoder\n",
        "    and context is the final output from encoder.\n",
        "\n",
        "    Args:\n",
        "        prefix (str): The prefix added to the layer names\n",
        "    \"\"\"\n",
        "    # create layers\n",
        "    context = tf.keras.layers.Input(shape=context_shape, dtype='float32',\n",
        "                                    name=f\"{prefix}_ctx2\")\n",
        "    inputs = tf.keras.layers.Input(shape=input_shape, dtype='float32',\n",
        "                                   name=f\"{prefix}_in2\")\n",
        "    attention = tf.keras.layers.MultiHeadAttention(name=f\"{prefix}_attn2\", **kwargs)\n",
        "    norm = tf.keras.layers.LayerNormalization(name=f\"{prefix}_norm2\")\n",
        "    add = tf.keras.layers.Add(name=f\"{prefix}_add2\")\n",
        "    # functional API to connect input to output\n",
        "    attout = attention(query=inputs, value=context, key=context)\n",
        "    outputs = norm(add([attout, inputs]))\n",
        "    # create model and return\n",
        "    model = tf.keras.Model(inputs=[(context, inputs)], outputs=outputs,\n",
        "                           name=f\"{prefix}_cross\")\n",
        "    return model\n",
        "\n",
        "seq_length = 20\n",
        "key_dim = 128\n",
        "num_heads = 8\n",
        "\n",
        "model = cross_attention(input_shape=(seq_length, key_dim),\n",
        "                        context_shape=(seq_length, key_dim),\n",
        "                        num_heads=num_heads, key_dim=key_dim)\n",
        "tf.keras.utils.plot_model(model, \"cross-attention.png\",\n",
        "                          show_shapes=True, show_dtype=True, show_layer_names=True,\n",
        "                          rankdir='BT', show_layer_activations=True)"
      ],
      "metadata": {
        "id": "uVZe0_1eNkfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feed forward models"
      ],
      "metadata": {
        "id": "Av8uC6b0Q_xG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def feed_forward(input_shape, model_dim, ff_dim, dropout=0.1, prefix=\"ff\"):\n",
        "    \"\"\"Feed-forward layers at transformer encoder and decoder. Assumes its\n",
        "    input is the output from an attention layer with add & norm, the output\n",
        "    is the output of one encoder or decoder block\n",
        "\n",
        "    Args:\n",
        "        model_dim (int): Output dimension of the feed-forward layer, which\n",
        "                         is also the output dimension of the encoder/decoder\n",
        "                         block\n",
        "        ff_dim (int): Internal dimension of the feed-forward layer\n",
        "        dropout (float): Dropout rate\n",
        "        prefix (str): The prefix added to the layer names\n",
        "    \"\"\"\n",
        "    # create layers\n",
        "    inputs = tf.keras.layers.Input(shape=input_shape, dtype='float32',\n",
        "                                   name=f\"{prefix}_in3\")\n",
        "    dense1 = tf.keras.layers.Dense(ff_dim, name=f\"{prefix}_ff1\", activation=\"relu\")\n",
        "    dense2 = tf.keras.layers.Dense(model_dim, name=f\"{prefix}_ff2\")\n",
        "    drop = tf.keras.layers.Dropout(dropout, name=f\"{prefix}_drop\")\n",
        "    add = tf.keras.layers.Add(name=f\"{prefix}_add3\")\n",
        "    # functional API to connect input to output\n",
        "    ffout = drop(dense2(dense1(inputs)))\n",
        "    norm = tf.keras.layers.LayerNormalization(name=f\"{prefix}_norm3\")\n",
        "    outputs = norm(add([inputs, ffout]))\n",
        "    # create model and return\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name=f\"{prefix}_ff\")\n",
        "    return model\n",
        "\n",
        "seq_length = 20\n",
        "key_dim = 128\n",
        "ff_dim = 512\n",
        "\n",
        "model = feed_forward(input_shape=(seq_length, key_dim),\n",
        "                     model_dim=key_dim, ff_dim=ff_dim)\n",
        "tf.keras.utils.plot_model(model, \"feedforward.png\",\n",
        "                          show_shapes=True, show_dtype=True, show_layer_names=True,\n",
        "                          rankdir='BT', show_layer_activations=True)"
      ],
      "metadata": {
        "id": "eLTrH4lJNoir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer Encoder and Decoder"
      ],
      "metadata": {
        "id": "QK5ozN8cahHO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def encoder(input_shape, key_dim, ff_dim, dropout=0.1, prefix=\"enc\", **kwargs):\n",
        "    \"\"\"One encoder unit. The input and output are in the same shape so we can\n",
        "    daisy chain multiple encoder units into one larger encoder\"\"\"\n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Input(shape=input_shape, dtype='float32', name=f\"{prefix}_in0\"),\n",
        "        self_attention(input_shape, prefix=prefix, key_dim=key_dim, mask=False, **kwargs),\n",
        "        feed_forward(input_shape, key_dim, ff_dim, dropout, prefix),\n",
        "    ], name=prefix)\n",
        "    return model\n",
        "\n",
        "\n",
        "seq_length = 20\n",
        "key_dim = 128\n",
        "ff_dim = 512\n",
        "num_heads = 8\n",
        "\n",
        "model = encoder(input_shape=(seq_length, key_dim), key_dim=key_dim, ff_dim=ff_dim,\n",
        "                num_heads=num_heads)\n",
        "tf.keras.utils.plot_model(model, \"encoder.png\",\n",
        "                          show_shapes=True, show_dtype=True, show_layer_names=True,\n",
        "                          rankdir='BT', show_layer_activations=True)"
      ],
      "metadata": {
        "id": "UdmWsl3-anR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decoder(input_shape, key_dim, ff_dim, dropout=0.1, prefix=\"dec\", **kwargs):\n",
        "    \"\"\"One decoder unit. The input and output are in the same shape so we can\n",
        "    daisy chain multiple decoder units into one larger decoder. The context\n",
        "    vector is also assumed to be the same shape for convenience\"\"\"\n",
        "    inputs = tf.keras.layers.Input(shape=input_shape, dtype='float32',\n",
        "                                   name=f\"{prefix}_in0\")\n",
        "    context = tf.keras.layers.Input(shape=input_shape, dtype='float32',\n",
        "                                    name=f\"{prefix}_ctx0\")\n",
        "    attmodel = self_attention(input_shape, key_dim=key_dim, mask=True,\n",
        "                              prefix=prefix, **kwargs)\n",
        "    crossmodel = cross_attention(input_shape, input_shape, key_dim=key_dim,\n",
        "                                 prefix=prefix, **kwargs)\n",
        "    ffmodel = feed_forward(input_shape, key_dim, ff_dim, dropout, prefix)\n",
        "    x = attmodel(inputs)\n",
        "    x = crossmodel([(context, x)])\n",
        "    output = ffmodel(x)\n",
        "    model = tf.keras.Model(inputs=[(inputs, context)], outputs=output, name=prefix)\n",
        "    return model\n",
        "\n",
        "\n",
        "seq_length = 20\n",
        "key_dim = 128\n",
        "ff_dim = 512\n",
        "num_heads = 8\n",
        "\n",
        "model = decoder(input_shape=(seq_length, key_dim), key_dim=key_dim, ff_dim=ff_dim,\n",
        "                num_heads=num_heads)\n",
        "tf.keras.utils.plot_model(model, \"decoder.png\",\n",
        "                          show_shapes=True, show_dtype=True, show_layer_names=True,\n",
        "                          rankdir='BT', show_layer_activations=True)"
      ],
      "metadata": {
        "id": "tLPXNdt4o39v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building a Transformer"
      ],
      "metadata": {
        "id": "93VDi5flqccT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transformer(num_layers, num_heads, seq_len, key_dim, ff_dim, vocab_size_src,\n",
        "                vocab_size_tgt, dropout=0.1, name=\"transformer\"):\n",
        "    embed_shape = (seq_len, key_dim)  # output shape of the positional embedding layer\n",
        "    # set up layers\n",
        "    input_enc = tf.keras.layers.Input(shape=(seq_len,), dtype=\"int32\",\n",
        "                                      name=\"encoder_inputs\")\n",
        "    input_dec = tf.keras.layers.Input(shape=(seq_len,), dtype=\"int32\",\n",
        "                                      name=\"decoder_inputs\")\n",
        "    embed_enc = PositionalEmbedding(seq_len, vocab_size_src, key_dim, name=\"embed_enc\")\n",
        "    embed_dec = PositionalEmbedding(seq_len, vocab_size_tgt, key_dim, name=\"embed_dec\")\n",
        "    encoders = [encoder(input_shape=embed_shape, key_dim=key_dim,\n",
        "                        ff_dim=ff_dim, dropout=dropout, prefix=f\"enc{i}\",\n",
        "                        num_heads=num_heads)\n",
        "                for i in range(num_layers)]\n",
        "    decoders = [decoder(input_shape=embed_shape, key_dim=key_dim,\n",
        "                        ff_dim=ff_dim, dropout=dropout, prefix=f\"dec{i}\",\n",
        "                        num_heads=num_heads)\n",
        "                for i in range(num_layers)]\n",
        "    final = tf.keras.layers.Dense(vocab_size_tgt, name=\"linear\")\n",
        "    # build output\n",
        "    x1 = embed_enc(input_enc)\n",
        "    x2 = embed_dec(input_dec)\n",
        "    for layer in encoders:\n",
        "        x1 = layer(x1)\n",
        "    for layer in decoders:\n",
        "        x2 = layer([x2, x1])\n",
        "    output = final(x2)\n",
        "    # XXX keep this try-except block\n",
        "    try:\n",
        "        del output._keras_mask\n",
        "    except AttributeError:\n",
        "        pass\n",
        "    model = tf.keras.Model(inputs=[input_enc, input_dec], outputs=output, name=name)\n",
        "    return model\n",
        "\n",
        "\n",
        "seq_len = 20\n",
        "num_layers = 4\n",
        "num_heads = 8\n",
        "key_dim = 128\n",
        "ff_dim = 512\n",
        "dropout = 0.1\n",
        "vocab_size_en = 10000\n",
        "vocab_size_fr = 20000\n",
        "model = transformer(num_layers, num_heads, seq_len, key_dim, ff_dim,\n",
        "                    vocab_size_en, vocab_size_fr, dropout)\n",
        "tf.keras.utils.plot_model(model, \"transformer.png\",\n",
        "                          show_shapes=True, show_dtype=True, show_layer_names=True,\n",
        "                          rankdir='BT', show_layer_activations=True)"
      ],
      "metadata": {
        "id": "FiVlydBGqeJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare the Transformer Model for Training"
      ],
      "metadata": {
        "id": "N_Hb_zsxulvH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    \"Custom learning rate for Adam optimizer\"\n",
        "    def __init__(self, key_dim, warmup_steps=4000):\n",
        "        super().__init__()\n",
        "        self.key_dim = key_dim\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.d = tf.cast(self.key_dim, tf.float32)\n",
        "\n",
        "    def __call__(self, step):\n",
        "        step = tf.cast(step, dtype=tf.float32)\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "        return tf.math.rsqrt(self.d) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "    def get_config(self):\n",
        "        # to make save and load a model using custom layer possible0\n",
        "        config = {\n",
        "            \"key_dim\": self.key_dim,\n",
        "            \"warmup_steps\": self.warmup_steps,\n",
        "        }\n",
        "        return config\n",
        "\n",
        "key_dim = 128\n",
        "lr = CustomSchedule(key_dim)\n",
        "optimizer = tf.keras.optimizers.Adam(lr, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
        "\n",
        "plt.plot(lr(tf.range(50000, dtype=tf.float32)))\n",
        "plt.ylabel('Learning Rate')\n",
        "plt.xlabel('Train Step')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "G2tRTtSdungU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def masked_loss(label, pred):\n",
        "    mask = label != 0\n",
        "\n",
        "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "        from_logits=True, reduction='none')\n",
        "    loss = loss_object(label, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss.dtype)\n",
        "    loss *= mask\n",
        "    loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
        "    return loss\n",
        "\n",
        "\n",
        "def masked_accuracy(label, pred):\n",
        "    pred = tf.argmax(pred, axis=2)\n",
        "    label = tf.cast(label, pred.dtype)\n",
        "    match = label == pred\n",
        "\n",
        "    mask = label != 0\n",
        "\n",
        "    match = match & mask\n",
        "\n",
        "    match = tf.cast(match, dtype=tf.float32)\n",
        "    mask = tf.cast(mask, dtype=tf.float32)\n",
        "    return tf.reduce_sum(match)/tf.reduce_sum(mask)"
      ],
      "metadata": {
        "id": "dzLtiJ_XEgR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size_en = 10000\n",
        "vocab_size_fr = 20000\n",
        "seq_len = 20\n",
        "num_layers = 4\n",
        "num_heads = 8\n",
        "key_dim = 128\n",
        "ff_dim = 512\n",
        "dropout = 0.1\n",
        "model = transformer(num_layers, num_heads, seq_len, key_dim, ff_dim,\n",
        "                    vocab_size_en, vocab_size_fr, dropout)\n",
        "lr = CustomSchedule(key_dim)\n",
        "optimizer = tf.keras.optimizers.Adam(lr, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
        "model.compile(loss=masked_loss, optimizer=optimizer, metrics=[masked_accuracy])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "7cb_tNqRE3Ql"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}