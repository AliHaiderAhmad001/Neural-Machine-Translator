# -*- coding: utf-8 -*-
"""train.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iAjbOC_0Y8t2SafAmfKp29Dkrn-7T56q
"""

import tensorflow as tf
from config import Config
from transformer import Transformer
from lr_schedule import LrSchedule
from transformer_callbacks import TransformerCallbacks
from loss_functions import cce_loss
from metrics import masked_accuracy

config = Config()

# Create the Transformer model
transformer = Transformer(config, config.source_vocab_size, config.target_vocab_size)

# Create the learning rate schedule
lr = LrSchedule(config.hidden_size, config.warmup_steps)

# Create the custom callbacks for monitoring and early stopping
callbacks = TransformerCallbacks(config)

# Create the Adam optimizer with the custom learning rate
optimizer = tf.keras.optimizers.Adam(
    lr,
    beta_1=0.9,
    beta_2=0.98,
    epsilon=1e-9
)

# Compile the Transformer model with the custom loss function and optimizer
transformer.compile(
    loss=cce_loss,
    optimizer=optimizer,
    metrics=[masked_accuracy]
)

# Train the Transformer model on the training dataset
# and validate it on the validation dataset
history = transformer.fit(
    train_ds,
    epochs=config.epochs,
    validation_data=val_ds,
    callbacks=callbacks
)